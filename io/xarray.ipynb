{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NetCDF\n",
    "\n",
    "A very efficient way to read and write NetCDF files is to use the [xarray](http://xarray.pydata.org/en/stable/) Python library, which can be viewed as a ND counterpart of the [pandas](http://pandas.pydata.org).\n",
    "\n",
    "## Reading NetCDF\n",
    "\n",
    "### Reading single file\n",
    "\n",
    "Reading NetCDF files is dones by using the `xarray.open_dataset` method, which returns a [xarray.Dataset](http://xarray.pydata.org/en/stable/data-structures.html#dataset) object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset>\n",
      "Dimensions:   (lat: 33, lon: 36, timestep: 1)\n",
      "Coordinates:\n",
      "  * lat       (lat) float32 20.0 21.25 22.5 23.75 25.0 ... 56.25 57.5 58.75 60.0\n",
      "  * lon       (lon) float32 -140.0 -137.5 -135.0 -132.5 ... -57.5 -55.0 -52.5\n",
      "  * timestep  (timestep) int32 372\n",
      "Data variables:\n",
      "    reftime   |S20 ...\n",
      "    v         (timestep, lat, lon) float32 ...\n",
      "    u         (timestep, lat, lon) float32 ...\n",
      "Attributes:\n",
      "    history:  Thu Jul 21 16:12:12 2016: ncks -A U500storm.nc V500storm.nc\\nTh...\n",
      "    NCO:      4.4.2\n"
     ]
    }
   ],
   "source": [
    "import xarray as xr\n",
    "\n",
    "data = xr.open_dataset('data/UV500storm.nc')\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading multiple files\n",
    "\n",
    "Often, a variable is stored in multiple NetCDF files (one file per year for instance). The `xarray.open_mfdataset` allows to open all the files in one row and to concatenate them along *record* dimension (`UNLIMITED` dimension, which is usually time) and the data coordinates.\n",
    "\n",
    "Below, the four `ISAS13` files are opened at once and are automatically concanated along the record dimension, hence leading to a dataset with 4 time steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset>\n",
      "Dimensions:  (depth: 152, time: 4)\n",
      "Coordinates:\n",
      "  * depth    (depth) float32 0.0 3.0 5.0 10.0 ... 1940.0 1960.0 1980.0 2000.0\n",
      "  * time     (time) datetime64[ns] 2012-01-15 2012-02-15 2012-03-15 2012-04-15\n",
      "Data variables:\n",
      "    TEMP     (time, depth) float32 dask.array<chunksize=(1, 152), meta=np.ndarray>\n",
      "Attributes:\n",
      "    script:   extract_prof.py\n",
      "    history:  Thu Jul 14 11:32:10 2016: ncks -O -3 ISAS13_20120115_fld_TEMP.n...\n",
      "    NCO:      \"4.5.5\"\n"
     ]
    }
   ],
   "source": [
    "data = xr.open_mfdataset(\"data/*ISAS*nc\", combine='by_coords')\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, complex models are often paralellized using the [Message Passing Interface (MPI)](https://fr.wikipedia.org/wiki/Message_Passing_Interface), in which each processor manages a subdomain. If each processor saves output in its sub-region, there will be as many output files as there are processors.\n",
    "`xarray` allows to reconstruct the global file by concatenating the subregional files according to their coordinates.\n",
    "\n",
    "<div class=\"alert alert-danger\">\n",
    "    <strong>Warning!</strong> This actually works only if the decomposition into subregions is regular, and if subfiles contain coordinates\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset>\n",
      "Dimensions:  (community: 3, time: 10, weight: 100, x: 32, y: 22)\n",
      "Coordinates:\n",
      "  * x        (x) int64 0 1 2 3 4 5 6 7 8 9 10 ... 22 23 24 25 26 27 28 29 30 31\n",
      "  * y        (y) int64 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21\n",
      "Dimensions without coordinates: community, time, weight\n",
      "Data variables:\n",
      "    OOPE     (time, y, x, community, weight) float32 dask.array<chunksize=(10, 11, 16, 3, 100), meta=np.ndarray>\n"
     ]
    }
   ],
   "source": [
    "data = xr.open_mfdataset(\"data/GYRE_OOPE*\", combine='by_coords')\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accessing dimensions, variables, attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset>\n",
      "Dimensions:   (lat: 33, lon: 36, timestep: 1)\n",
      "Coordinates:\n",
      "  * lat       (lat) float32 20.0 21.25 22.5 23.75 25.0 ... 56.25 57.5 58.75 60.0\n",
      "  * lon       (lon) float32 -140.0 -137.5 -135.0 -132.5 ... -57.5 -55.0 -52.5\n",
      "  * timestep  (timestep) int32 372\n",
      "Data variables:\n",
      "    reftime   |S20 ...\n",
      "    v         (timestep, lat, lon) float32 ...\n",
      "    u         (timestep, lat, lon) float32 ...\n",
      "Attributes:\n",
      "    history:  Thu Jul 21 16:12:12 2016: ncks -A U500storm.nc V500storm.nc\\nTh...\n",
      "    NCO:      4.4.2\n"
     ]
    }
   ],
   "source": [
    "data = xr.open_dataset(\"data/UV500storm.nc\")\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dimensions\n",
    "\n",
    "Recovering dimensions is dony by accessing the `dims` attribute of the dataset, which returns a `dictionary`, the `keys` of which are the dataset dimension names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dim lat n= 33\n",
      "dim lon n= 36\n",
      "dim timestep n= 1\n",
      "33\n"
     ]
    }
   ],
   "source": [
    "# Recovering the number of values along a dimension\n",
    "for k, v in data.dims.items():\n",
    "    print('dim', k, 'n=', v)\n",
    "print(data.dims['lat'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variables\n",
    "\n",
    "Variables can be accessed by using the `data_vars` attribute, which returns a `dictionary`,  the `keys` of which are the dataset variable names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "var reftime shape ()\n",
      "var v shape (1, 33, 36)\n",
      "var u shape (1, 33, 36)\n"
     ]
    }
   ],
   "source": [
    "var = data.data_vars\n",
    "for k, v in var.items():\n",
    "    print('var', k, 'shape', v.shape)   \n",
    "\n",
    "u = data.data_vars['u']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that data variables can also be accessed by using variable name as the key to the dataset object as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = data['v']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the `data_vars` attribute is not used. \n",
    "\n",
    "In the above, the variable is extracted into a \n",
    "[xarray.DataArray](http://xarray.pydata.org/en/stable/data-structures.html#dataarray) object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To recover the variable as a `numpy` array, the `values` attribute can be used. In this case, missing values are set to `NaN`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = data['v']\n",
    "print(type(v))\n",
    "v = v.values\n",
    "print(type(v))\n",
    "\n",
    "v.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to obtain a masked array instead, use the `to_masked_array()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "<class 'numpy.ma.core.MaskedArray'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.7472397420416235"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = data['v']\n",
    "print(type(v))\n",
    "v = v.to_masked_array()\n",
    "print(type(v))\n",
    "\n",
    "v.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Time management\n",
    "\n",
    "By default, the time variable is detected by `xarray` by using the NetCDF attributes, and is converted into a human time. This is done by xarray by using the [cftime](https://pypi.org/project/cftime/) module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2012-01-15T00:00:00.000000000' '2012-02-15T00:00:00.000000000'\n",
      " '2012-03-15T00:00:00.000000000' '2012-04-15T00:00:00.000000000']\n"
     ]
    }
   ],
   "source": [
    "data = xr.open_mfdataset(\"data/*ISAS*\", combine='by_coords')\n",
    "print(data['time'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, the user can access the `year`, `month`, `day`, `hour`, `minute`, `second`, `microsecond`, `nanosecond`, `date`, `time`, `dayofyear`, `weekofyear`, `dayofweek`, `quarter` as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2012 2012 2012 2012]\n",
      "[1 2 3 4]\n",
      "[15 15 15 15]\n",
      "[ 15  46  75 106]\n"
     ]
    }
   ],
   "source": [
    "print(data['time.year'].values)\n",
    "print(data['time.month'].values)\n",
    "print(data['time.day'].values)\n",
    "print(data['time.dayofyear'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <strong>Warning</strong> Replace <i>time</i> by the name of your time variable (<i>time_counter</i> in NEMO for instance)\n",
    "</div>\n",
    "\n",
    "If the user does not want `xarray` to convert time into a human date, set the `decode_times` argument to False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22659. 22690. 22719. 22750.]\n"
     ]
    }
   ],
   "source": [
    "data = xr.open_mfdataset(\"data/*ISAS*\", combine='by_coords', decode_times=False)\n",
    "print(data['time'].values)\n",
    "# print(data['time.year'].values)  #  crashes because time is a float, not a date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attributes\n",
    "\n",
    "To get variable attributes, use the `attrs` attribute, which exists for DataSet and DataArray objects. It returns a `dictionaray` containing the attribute names and values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attr script val extract_prof.py\n",
      "attr history val Thu Jul 14 11:32:10 2016: ncks -O -3 ISAS13_20120115_fld_TEMP.nc ISAS13_20120115_fld_TEMP.nc\n",
      "attr NCO val \"4.5.5\"\n",
      "Thu Jul 14 11:32:10 2016: ncks -O -3 ISAS13_20120115_fld_TEMP.nc ISAS13_20120115_fld_TEMP.nc\n"
     ]
    }
   ],
   "source": [
    "# Recovering global (file) attributes\n",
    "for k, v in data.attrs.items():\n",
    "    print('attr', k, 'val', v)\n",
    "print(data.attrs['history'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attr units val days since 1950-01-01T00:00:00Z\n",
      "days since 1950-01-01T00:00:00Z\n"
     ]
    }
   ],
   "source": [
    "time = data['time']\n",
    "# Recovering variable attributes\n",
    "for k, v in time.attrs.items():\n",
    "    print('attr', k, 'val', v)\n",
    "print(time.attrs['units'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing\n",
    "\n",
    "As in `pandas`, there is 2 ways to extract part of a dataset. Let's consider the ISAS dataset, which contains 152 vertical levels unevenly from 0 to 2000m. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.DataArray 'depth' (depth: 152)>\n",
      "array([   0.,    3.,    5.,   10.,   15.,   20.,   25.,   30.,   35.,   40.,\n",
      "         45.,   50.,   55.,   60.,   65.,   70.,   75.,   80.,   85.,   90.,\n",
      "         95.,  100.,  110.,  120.,  130.,  140.,  150.,  160.,  170.,  180.,\n",
      "        190.,  200.,  210.,  220.,  230.,  240.,  250.,  260.,  270.,  280.,\n",
      "        290.,  300.,  310.,  320.,  330.,  340.,  350.,  360.,  370.,  380.,\n",
      "        390.,  400.,  410.,  420.,  430.,  440.,  450.,  460.,  470.,  480.,\n",
      "        490.,  500.,  510.,  520.,  530.,  540.,  550.,  560.,  570.,  580.,\n",
      "        590.,  600.,  610.,  620.,  630.,  640.,  650.,  660.,  670.,  680.,\n",
      "        690.,  700.,  710.,  720.,  730.,  740.,  750.,  760.,  770.,  780.,\n",
      "        790.,  800.,  820.,  840.,  860.,  880.,  900.,  920.,  940.,  960.,\n",
      "        980., 1000., 1020., 1040., 1060., 1080., 1100., 1120., 1140., 1160.,\n",
      "       1180., 1200., 1220., 1240., 1260., 1280., 1300., 1320., 1340., 1360.,\n",
      "       1380., 1400., 1420., 1440., 1460., 1480., 1500., 1520., 1540., 1560.,\n",
      "       1580., 1600., 1620., 1640., 1660., 1680., 1700., 1720., 1740., 1760.,\n",
      "       1780., 1800., 1820., 1840., 1860., 1880., 1900., 1920., 1940., 1960.,\n",
      "       1980., 2000.], dtype=float32)\n",
      "Coordinates:\n",
      "  * depth    (depth) float32 0.0 3.0 5.0 10.0 ... 1940.0 1960.0 1980.0 2000.0\n"
     ]
    }
   ],
   "source": [
    "data = xr.open_mfdataset('data/*ISAS*', combine='by_coords')\n",
    "print(data['depth'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting using indexes\n",
    "\n",
    "To extract the ten first level and the first to time steps, the `isel` method should be used, which can be applied on either `DataSet` or `DataArray`.\n",
    "\n",
    "<div class='alert alert-info'>\n",
    "    <strong>Note</strong> It is the xarray counterpart of the Pandas iloc method\n",
    "</div>        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset>\n",
      "Dimensions:  (depth: 10, time: 2)\n",
      "Coordinates:\n",
      "  * depth    (depth) float32 0.0 3.0 5.0 10.0 15.0 20.0 25.0 30.0 35.0 40.0\n",
      "  * time     (time) datetime64[ns] 2012-01-15 2012-02-15\n",
      "Data variables:\n",
      "    TEMP     (time, depth) float32 dask.array<chunksize=(1, 10), meta=np.ndarray>\n",
      "Attributes:\n",
      "    script:   extract_prof.py\n",
      "    history:  Thu Jul 14 11:32:10 2016: ncks -O -3 ISAS13_20120115_fld_TEMP.n...\n",
      "    NCO:      \"4.5.5\"\n",
      "<xarray.DataArray 'TEMP' (time: 2, depth: 10)>\n",
      "dask.array<getitem, shape=(2, 10), dtype=float32, chunksize=(1, 10), chunktype=numpy.ndarray>\n",
      "Coordinates:\n",
      "  * depth    (depth) float32 0.0 3.0 5.0 10.0 15.0 20.0 25.0 30.0 35.0 40.0\n",
      "  * time     (time) datetime64[ns] 2012-01-15 2012-02-15\n"
     ]
    }
   ],
   "source": [
    "data_s = data.isel(time=slice(0, 2), depth=range(0, 10))\n",
    "print(data_s)\n",
    "\n",
    "temp = data['TEMP'].isel(time=slice(0, 2), depth=range(0, 10))\n",
    "print(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting using values\n",
    "\n",
    "To extract the data between 100m and 500m and for a given period, the `sel` method should be used, which can be applied on either `DataSet` or `DataArray`. It allows use values rather than indexes.\n",
    "\n",
    "<div class='alert alert-info'>\n",
    "    <strong>Note</strong> It is the xarray counterpart of the Pandas loc method\n",
    "</div>     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset>\n",
      "Dimensions:  (depth: 41, time: 2)\n",
      "Coordinates:\n",
      "  * depth    (depth) float32 100.0 110.0 120.0 130.0 ... 470.0 480.0 490.0 500.0\n",
      "  * time     (time) datetime64[ns] 2012-01-15 2012-02-15\n",
      "Data variables:\n",
      "    TEMP     (time, depth) float32 dask.array<chunksize=(1, 41), meta=np.ndarray>\n",
      "Attributes:\n",
      "    script:   extract_prof.py\n",
      "    history:  Thu Jul 14 11:32:10 2016: ncks -O -3 ISAS13_20120115_fld_TEMP.n...\n",
      "    NCO:      \"4.5.5\"\n"
     ]
    }
   ],
   "source": [
    "data_s = data.sel(time=slice('2012-01-15', '2012-02-15'), depth=slice(100, 500))\n",
    "print(data_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating NetCDF\n",
    "\n",
    "An easy way to write a NetCDF is to create a `DataSet` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from cftime import utime\n",
    "\n",
    "nx = 10\n",
    "ny = 20\n",
    "ntime = 5\n",
    "x = np.arange(nx)\n",
    "y = np.arange(ny)\n",
    "\n",
    "data = np.random.rand(ntime, ny, nx) - 0.5\n",
    "data = np.ma.masked_where(data < 0, data)\n",
    "\n",
    "# converts time into date\n",
    "time = np.arange(ntime)\n",
    "date = utime('days since 1900-01-01 00:00:00').num2date(time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, init an empty `Dataset` object by calling the [xarray.Dataset](http://xarray.pydata.org/en/stable/generated/xarray.Dataset.html)method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.Dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, add to the dataset the variables and coordinates. Note that they should be provided as a tuple that contains two elements:\n",
    "- A list of dimension names\n",
    "- The numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5,)\n",
      "(5, 20, 10)\n"
     ]
    }
   ],
   "source": [
    "print(time.shape)\n",
    "print(data.shape)\n",
    "\n",
    "ds['data'] = (['time', 'y', 'x'], data)\n",
    "ds['x'] = (['x'], x)\n",
    "ds['y'] = (['y'], y)\n",
    "ds['time'] = (['time'], date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, add the dataset and variable attributes as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Set file global attributes (file directory name + date)\n",
    "ds.attrs['script'] = os.getcwd()\n",
    "ds.attrs['date'] = str(datetime.today())\n",
    "\n",
    "ds['data'].attrs['description'] = 'Random draft'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, create the NetCDF file by using the [to_netcdf](http://xarray.pydata.org/en/stable/generated/xarray.Dataset.to_netcdf.html) method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.to_netcdf('data/example.nc', unlimited_dims='time', format='NETCDF4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that xarray automatically writes the `_FilValue` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
